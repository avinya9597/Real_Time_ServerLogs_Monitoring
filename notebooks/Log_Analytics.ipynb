{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "24560597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# !pip install mrjob\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea032d88",
   "metadata": {},
   "source": [
    "## Data Cleaning - Parse the logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb97acd",
   "metadata": {},
   "source": [
    "#### Provide the log pattern as per the input data (logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c8b72e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pattern = r'(?P<ip>\\d+\\.\\d+\\.\\d+\\.\\d+) - - \\[(?P<timestamp>.*?)\\] \"(?P<method>\\S+) (?P<url>\\S+) (?P<http_version>.*?)\" (?P<status_code>\\d+) (?P<response_size>\\d+) \"(?P<referrer>.*?)\" \"(?P<user_agent>.*?)\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cc61eda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse log entries\n",
    "def parse_log_entry(log_entry):\n",
    "    match = re.match(log_pattern, log_entry)\n",
    "    if match:\n",
    "        return match.groupdict()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to read log file and parse log entries\n",
    "def parse_log_file(file_path):\n",
    "    parsed_logs = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            log_entry = line.strip()\n",
    "            parsed_log_entry = parse_log_entry(log_entry)\n",
    "            if parsed_log_entry:\n",
    "                parsed_logs.append(parsed_log_entry)\n",
    "    return parsed_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e2ec664d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = '../Downloads/log_file.log'\n",
    "parsed_logs = parse_log_file(file_path)\n",
    "#     for log in parsed_logs:\n",
    "#         print(log)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6e1159eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ip': '87.116.74.253',\n",
       " 'timestamp': '28/Feb/2022:14:04:44 +0200',\n",
       " 'method': 'POST',\n",
       " 'url': '/wp-admin/admin-ajax.php',\n",
       " 'http_version': 'HTTP/2.0',\n",
       " 'status_code': '200',\n",
       " 'response_size': '47',\n",
       " 'referrer': 'https://nargile.bg/wp-admin/admin.php?page=wc-settings',\n",
       " 'user_agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.109 Safari/537.36'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_logs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d658e090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(parsed_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daafa501",
   "metadata": {},
   "source": [
    "## MapReduce Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef19b81",
   "metadata": {},
   "source": [
    "### Hadoop MapReduce logic  identify patterns and trends. MapReduce jobs  count the occurrence of specific log messages, identify common error patterns, or detect unusual access patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8dc83310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_function(log):\n",
    "    # Extract relevant information from the log\n",
    "    url = log['url']\n",
    "    status_code = log['status_code']\n",
    "    \n",
    "    # Emit key-value pairs (pattern, 1)\n",
    "    if status_code != '200':\n",
    "        yield ('error', 1)\n",
    "    \n",
    "    if '/wp-content/' in url:\n",
    "        yield ('wp_content', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c8cfe58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_function(pairs):\n",
    "    # Aggregate counts for each pattern\n",
    "    pattern_counts = defaultdict(int)\n",
    "    for pattern, count in pairs:\n",
    "        pattern_counts[pattern] += count\n",
    "    return pattern_counts.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "60a369ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed logs\n",
    "logs = parsed_logs\n",
    "\n",
    "# Map phase\n",
    "mapped_pairs = [result for log in logs for result in map_function(log)]\n",
    "\n",
    "# Reduce phase\n",
    "reduced_output = reduce_function(mapped_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5126b24f",
   "metadata": {},
   "source": [
    "## Output Response from MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d61b9d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wp_content: 1767145\n",
      "error: 63234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Output the result\n",
    "for pattern, count in tqdm(reduced_output):\n",
    "    print(f'{pattern}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cbbb6e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wp_content', 1),\n",
       " ('wp_content', 1),\n",
       " ('wp_content', 1),\n",
       " ('wp_content', 1),\n",
       " ('wp_content', 1),\n",
       " ('wp_content', 1),\n",
       " ('error', 1),\n",
       " ('wp_content', 1),\n",
       " ('wp_content', 1),\n",
       " ('wp_content', 1)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_pairs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fd74b584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('wp_content', 1767145), ('error', 63234)])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8303b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_function_anomaly(log):\n",
    "    # Extract relevant information from the log\n",
    "    timestamp = log['timestamp']\n",
    "    status_code = log['status_code']\n",
    "    response_size = int(log['response_size'])\n",
    "    \n",
    "    # Extract hour from timestamp\n",
    "    hour = timestamp.split(':')[1]\n",
    "    \n",
    "    # Emit key-value pairs (hour, (1, response_size, status_code))\n",
    "    yield (hour, (1, response_size, status_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "518d5667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_function_anomaly(pairs):\n",
    "    # Aggregate counts, total response size, and error counts for each hour\n",
    "    hour_metrics = defaultdict(lambda: (0, 0, 0))  # (count, total_response_size, error_count)\n",
    "    for hour, metrics in pairs:\n",
    "        count, response_size, status_code = metrics\n",
    "        hour_metrics[hour] = (hour_metrics[hour][0] + count,\n",
    "                              hour_metrics[hour][1] + response_size,\n",
    "                              hour_metrics[hour][2] + (1 if status_code != '200' else 0))\n",
    "    return hour_metrics.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b3f6fc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed logs\n",
    "logs = parsed_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7ae31185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map phase for anomaly detection\n",
    "mapped_pairs_anomaly = [result for log in logs for result in map_function_anomaly(log)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "63314140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('14', (1, 47, '200')),\n",
       " ('14', (1, 47, '200')),\n",
       " ('14', (1, 262929, '200')),\n",
       " ('14', (1, 992, '200')),\n",
       " ('14', (1, 308, '200'))]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_pairs_anomaly[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "291141af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce phase for anomaly detection\n",
    "reduced_output_anomaly = reduce_function_anomaly(mapped_pairs_anomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cc3adddd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('14', (137344, 6213820968, 4090)), ('15', (175472, 8095543416, 5157)), ('16', (171536, 7450838319, 4368)), ('17', (171171, 7501961016, 3972)), ('18', (137404, 5907974861, 3483)), ('19', (143527, 5705280593, 3803)), ('20', (135327, 5372765135, 3842)), ('21', (133572, 5263420549, 3298)), ('22', (130329, 5251491647, 3173)), ('23', (107173, 4299619857, 2485)), ('00', (65452, 2963020097, 2069)), ('01', (52358, 2570766753, 1679)), ('02', (34662, 1844177913, 991)), ('03', (20482, 1401168315, 923)), ('04', (10338, 629913836, 488)), ('05', (14208, 1292916672, 2928)), ('06', (10292, 570377406, 354)), ('07', (18467, 985466662, 728)), ('08', (42711, 1730509776, 1112)), ('09', (62675, 2619602323, 1808)), ('10', (66239, 2780139074, 1957)), ('11', (89203, 4449049527, 3285)), ('12', (129358, 6011936639, 4111)), ('13', (103919, 4933080283, 3130))])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_output_anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cec4e2b",
   "metadata": {},
   "source": [
    "#### The output indicates that for the hour , the average response size and the error rate giving insights into the behavior of your system over time. This information can be valuable for identifying anomalies or trends in the log data.\n",
    "\n",
    "#### Hour: The hour of the day for which the metrics are calculated.\n",
    "#### Average Response Size: The average size of responses received during that hour.\n",
    "#### Error Rate: The percentage of requests that resulted in an error during that hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "63e55100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hour: 14, Average Response Size: 45242.755184063375, Error Rate: 2.9779240447343898%\n",
      "Hour: 15, Average Response Size: 46135.813212364366, Error Rate: 2.938930427646576%\n",
      "Hour: 16, Average Response Size: 43436.00363189068, Error Rate: 2.5464042533345768%\n",
      "Hour: 17, Average Response Size: 43827.28976286871, Error Rate: 2.320486531012847%\n",
      "Hour: 18, Average Response Size: 42997.109698407614, Error Rate: 2.5348607027451893%\n",
      "Hour: 19, Average Response Size: 39750.57371086973, Error Rate: 2.649675670779714%\n",
      "Hour: 20, Average Response Size: 39702.09296740488, Error Rate: 2.8390491180621753%\n",
      "Hour: 21, Average Response Size: 39405.11895457132, Error Rate: 2.4690803461803372%\n",
      "Hour: 22, Average Response Size: 40294.114487182436, Error Rate: 2.4346078002593434%\n",
      "Hour: 23, Average Response Size: 40118.49866104336, Error Rate: 2.318681011075551%\n",
      "Hour: 00, Average Response Size: 45270.1230978427, Error Rate: 3.1610951537004217%\n",
      "Hour: 01, Average Response Size: 49099.78901027541, Error Rate: 3.2067687841399595%\n",
      "Hour: 02, Average Response Size: 53204.601956032544, Error Rate: 2.859038716750332%\n",
      "Hour: 03, Average Response Size: 68409.74099209062, Error Rate: 4.506395859779318%\n",
      "Hour: 04, Average Response Size: 60931.885857999616, Error Rate: 4.720448829560843%\n",
      "Hour: 05, Average Response Size: 90999.2027027027, Error Rate: 20.60810810810811%\n",
      "Hour: 06, Average Response Size: 55419.491449669644, Error Rate: 3.439564710454722%\n",
      "Hour: 07, Average Response Size: 53363.65744300644, Error Rate: 3.942167108896951%\n",
      "Hour: 08, Average Response Size: 40516.723467022544, Error Rate: 2.603544754278757%\n",
      "Hour: 09, Average Response Size: 41796.60666932589, Error Rate: 2.8847227762265657%\n",
      "Hour: 10, Average Response Size: 41971.332206102146, Error Rate: 2.954452814806987%\n",
      "Hour: 11, Average Response Size: 49875.55942064729, Error Rate: 3.6826115713597076%\n",
      "Hour: 12, Average Response Size: 46475.182354396326, Error Rate: 3.1780021336136923%\n",
      "Hour: 13, Average Response Size: 47470.43642644752, Error Rate: 3.011961239041946%\n"
     ]
    }
   ],
   "source": [
    "# Output the result\n",
    "for hour, metrics in reduced_output_anomaly:\n",
    "    count, total_response_size, error_count = metrics\n",
    "    average_response_size = total_response_size / count if count > 0 else 0\n",
    "    error_rate = (error_count / count) * 100 if count > 0 else 0\n",
    "    print(f'Hour: {hour}, Average Response Size: {average_response_size}, Error Rate: {error_rate}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fee960b",
   "metadata": {},
   "source": [
    "### Define thresholds for error rate and response size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "53afa827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hour: 03, Average Response Size: 68409.74099209062, Error Rate: 4.506395859779318%, Anomaly Detected: High Average Response Size\n",
      "Hour: 04, Average Response Size: 60931.885857999616, Error Rate: 4.720448829560843%, Anomaly Detected: High Average Response Size\n",
      "Hour: 05, Average Response Size: 90999.2027027027, Error Rate: 20.60810810810811%, Anomaly Detected: High Error Rate\n"
     ]
    }
   ],
   "source": [
    "# Define thresholds\n",
    "error_rate_threshold = 5  # 5% error rate threshold\n",
    "response_size_lower_threshold = 20000  # Lower response size threshold\n",
    "response_size_upper_threshold = 60000  # Upper response size threshold\n",
    "\n",
    "# Output the result with anomaly detection\n",
    "for hour, metrics in reduced_output_anomaly:\n",
    "    count, total_response_size, error_count = metrics\n",
    "    average_response_size = total_response_size / count if count > 0 else 0\n",
    "    error_rate = (error_count / count) * 100 if count > 0 else 0\n",
    "    \n",
    "    # Check for anomalies\n",
    "    anomaly_detected = False\n",
    "    if error_rate > error_rate_threshold:\n",
    "        anomaly_detected = True\n",
    "        anomaly_type = 'High Error Rate'\n",
    "    elif average_response_size < response_size_lower_threshold:\n",
    "        anomaly_detected = True\n",
    "        anomaly_type = 'Low Average Response Size'\n",
    "    elif average_response_size > response_size_upper_threshold:\n",
    "        anomaly_detected = True\n",
    "        anomaly_type = 'High Average Response Size'\n",
    "    \n",
    "    # Print the result\n",
    "    if anomaly_detected:\n",
    "        print(f'Hour: {hour}, Average Response Size: {average_response_size}, Error Rate: {error_rate}%, Anomaly Detected: {anomaly_type}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ac0920",
   "metadata": {},
   "source": [
    "#### Hour 3 and Hour 4 are flagged as anomalies due to their high average response sizes.\n",
    "Hour 5 is flagged as an anomaly due to its high error rate.\n",
    "This indicates that during these hours, there were significant deviations from normal behavior, either in terms of response size or error rate, which may require further investigation or action. This approach helps in proactively identifying potential issues or unusual patterns in the log data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d16cab",
   "metadata": {},
   "source": [
    "## To implement real-time monitoring in conjunction with MapReduce for near-real-time analysis of log data, you can integrate technologies like Apache Kafka for streaming log data and Apache Storm for real-time processing. Here's how you can extend the existing implementation to include real-time monitoring:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32570f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kafka-python\n",
    "from kafka import KafkaConsumer\n",
    "from kafka import KafkaProducer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18606abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Kafka producer to send parsed log messages\n",
    "producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "\n",
    "# Produce parsed log messages to the 'log-analytics' Kafka topic\n",
    "for log_data in parsed_logs:\n",
    "    # Convert log data to JSON format\n",
    "    message = json.dumps(log_data).encode('utf-8')\n",
    "    \n",
    "    # Send log message to Kafka\n",
    "    producer.send('log-analytics', value=message)\n",
    "\n",
    "# Flush the producer to ensure all messages are sent\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a9a9c429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from collections import defaultdict\n",
    "from storm import BasicBolt, Storm\n",
    "\n",
    "# Set up Kafka consumer to ingest streaming log data\n",
    "consumer = KafkaConsumer('log-analytics', group_id='log-group', bootstrap_servers=['localhost:9092'])\n",
    "\n",
    "# Define thresholds\n",
    "error_rate_threshold = 5  # 5% error rate threshold\n",
    "response_size_lower_threshold = 20000  # Lower response size threshold\n",
    "response_size_upper_threshold = 60000  # Upper response size threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "32376878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to determine if log data represents an anomaly\n",
    "def is_anomaly(log_data):\n",
    "    error_rate = (log_data['error_count'] / log_data['request_count']) * 100 if log_data['request_count'] > 0 else 0\n",
    "    average_response_size = log_data['total_response_size'] / log_data['request_count'] if log_data['request_count'] > 0 else 0\n",
    "    \n",
    "    if error_rate > error_rate_threshold or average_response_size < response_size_lower_threshold or average_response_size > response_size_upper_threshold:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "61776607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to emit an alert for the detected anomaly\n",
    "def emit_alert(log_data):\n",
    "    print(\"Anomaly detected!\")\n",
    "    print(\"Log data:\", log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4db0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Storm bolt to perform real-time processing\n",
    "class LogProcessingBolt(BasicBolt):\n",
    "    def process(self, tup):\n",
    "        log_data = tup.values[0]\n",
    "        \n",
    "        # Perform real-time processing and anomaly detection logic here\n",
    "        # Update metrics\n",
    "         hour_metrics[log_data['hour']] = (hour_metrics[log_data['hour']][0] + 1,\n",
    "                                      hour_metrics[log_data['hour']][1] + int(log_data['response_size']),\n",
    "                                      hour_metrics[log_data['hour']][2] + (1 if log_data['status_code'] != '200' else 0))\n",
    "        \n",
    "        # If an anomaly is detected, emit an alert\n",
    "        if is_anomaly(log_data):\n",
    "            emit_alert(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f07290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Apache Storm topology\n",
    "storm = Storm()\n",
    "storm.add_bolt(LogProcessingBolt)\n",
    "\n",
    "# Continuously consume streaming log data\n",
    "for message in consumer:\n",
    "    log_data = message.value  # Assuming log data is in JSON format\n",
    "    \n",
    "    # Emit log data to Storm for processing\n",
    "    storm.emit({'log_data': log_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0777eeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************************"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
