{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1ab74db",
   "metadata": {},
   "source": [
    "<!-- We've integrated Apache Storm into the system by defining a Storm bolt (LogProcessingBolt) to perform real-time processing of log data.\n",
    "The Kafka consumer continuously consumes log data from the Kafka topic, and each log message is emitted to the Storm topology for processing.\n",
    "Inside the Storm bolt, real-time processing and anomaly detection logic can be implemented. If an anomaly is detected, an alert can be emitted using the emit_alert function.\n",
    "This setup allows for scalable, fault-tolerant, and real-time processing of log data using Apache Storm in conjunction with Apache Kafka. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d982a11",
   "metadata": {},
   "source": [
    "## We've integrated Apache Storm into the system by defining a Storm bolt (LogProcessingBolt) to perform real-time processing of log data.\n",
    "## The Kafka consumer continuously consumes log data from the Kafka topic, and each log message is emitted to the Storm topology for processing.\n",
    "## Inside the Storm bolt, real-time processing and anomaly detection logic can be implemented. If an anomaly is detected, an alert can be emitted using the emit_alert function.\n",
    "## This setup allows for scalable, fault-tolerant, and real-time processing of log data using Apache Storm in conjunction with Apache Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8dd35b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kafka-python\n",
    "# !pip install storm\n",
    "from kafka import KafkaProducer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "857fa79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from collections import defaultdict\n",
    "from storm import BasicBolt, Storm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c4cefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Kafka consumer to ingest streaming log data\n",
    "consumer = KafkaConsumer('log-analytics', group_id='log-group', bootstrap_servers=['localhost:9092'])\n",
    "\n",
    "# Define thresholds\n",
    "error_rate_threshold = 5  # 5% error rate threshold\n",
    "response_size_lower_threshold = 20000  # Lower response size threshold\n",
    "response_size_upper_threshold = 60000  # Upper response size threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5c8af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to determine if log data represents an anomaly\n",
    "def is_anomaly(log_data):\n",
    "    error_rate = (log_data['error_count'] / log_data['request_count']) * 100 if log_data['request_count'] > 0 else 0\n",
    "    average_response_size = log_data['total_response_size'] / log_data['request_count'] if log_data['request_count'] > 0 else 0\n",
    "    \n",
    "    if error_rate > error_rate_threshold or average_response_size < response_size_lower_threshold or average_response_size > response_size_upper_threshold:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5062e265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to emit an alert for the detected anomaly\n",
    "def emit_alert(log_data):\n",
    "    print(\"Anomaly detected!\")\n",
    "    print(\"Log data:\", log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d099191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Storm bolt to perform real-time processing\n",
    "class LogProcessingBolt(BasicBolt):\n",
    "    def process(self, tup):\n",
    "        log_data = tup.values[0]\n",
    "        \n",
    "        # Perform real-time processing and anomaly detection logic here\n",
    "        # Update metrics\n",
    "         hour_metrics[log_data['hour']] = (hour_metrics[log_data['hour']][0] + 1,\n",
    "                                      hour_metrics[log_data['hour']][1] + int(log_data['response_size']),\n",
    "                                      hour_metrics[log_data['hour']][2] + (1 if log_data['status_code'] != '200' else 0))\n",
    "        \n",
    "        # If an anomaly is detected, emit an alert\n",
    "        if is_anomaly(log_data):\n",
    "            emit_alert(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3668e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Apache Storm topology\n",
    "storm = Storm()\n",
    "storm.add_bolt(LogProcessingBolt)\n",
    "\n",
    "# Continuously consume streaming log data\n",
    "for message in consumer:\n",
    "    log_data = message.value  # Assuming log data is in JSON format\n",
    "    \n",
    "    # Emit log data to Storm for processing\n",
    "    storm.emit({'log_data': log_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc68b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#***************************************************************************************************************************"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
